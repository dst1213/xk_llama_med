{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SRfm95yIahrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "程序断之后，临时数据会丢失(比如git clone的数据会丢失，而且Google drive的也会丢失！！！），训练可以放到notebook里边，参考glm的，不会丢失。drive/MyDrive/Colab Notebooks"
      ],
      "metadata": {
        "id": "-7n_YZnq4KaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "注意：在“修改”里边把笔记本设置改为GPU后，运行时是会清空之前的数据，需要重新上传，很麻烦，但是直接用GPU又会浪费GPU额度（每次连续12个小时，下次是12个小时之后）\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1MkgQ7wOajk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "！！！先把lora权重搞到谷歌硬盘Google drive上面"
      ],
      "metadata": {
        "id": "cPK6R-mybOwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##*挂载谷歌硬盘*\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKGU0Ul-MC98",
        "outputId": "2a6a551e-bda0-48d8-a68c-2cd2e1f71fc2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/Colab Notebooks\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R2uYnNuPF_A",
        "outputId": "5e64ea58-8d0b-4277-cf03-6d07beecc8b2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdvertiseGen.tar.gz\t\t       huatuo-llama\t  Untitled0.ipynb\n",
            "“Chinese-Vicuna-generate.ipynb”的副本  llama_model.ipynb\n",
            "“Colab_ChatGLM.ipynb”的副本\t       Untitled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNYLoG8BPW4P",
        "outputId": "f16379d9-03ec-4c8e-ddc9-6e062dc7e577"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1oid_vHbskE",
        "outputId": "a5df2d9b-10c2-4d37-9207-339bc3351560"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Huatuo-Llama-Med-Chinese'...\n",
            "remote: Enumerating objects: 124, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
            "remote: Total 124 (delta 54), reused 93 (delta 30), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (124/124), 2.54 MiB | 16.27 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"Huatuo-Llama-Med-Chinese\")"
      ],
      "metadata": {
        "id": "BSffqwKCbzj8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r ./requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOhz-_JGbyJR",
        "outputId": "b03abb9a-742f-4c60-9806-363fa4e07777"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/peft.git (from -r ./requirements.txt (line 8))\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-_fzx1yvw\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-_fzx1yvw\n",
            "  Resolved https://github.com/huggingface/peft.git to commit b1059b73aab9043b118ff19b0cf96263ea86248a\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/huggingface/transformers.git (from -r ./requirements.txt (line 9))\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-_rouj8ew\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-_rouj8ew\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit dbc12269ed5546b2da9236b9f1078b95b6a4d3d5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 2)) (1.4.4)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.38.1-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black\n",
            "  Downloading black-23.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.28.3-py3-none-any.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.15.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->-r ./requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r ./requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r ./requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r ./requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r ./requirements.txt (line 1)) (2.0.0+cu118)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black->-r ./requirements.txt (line 4)) (8.1.3)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black->-r ./requirements.txt (line 4)) (2.0.1)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black->-r ./requirements.txt (line 4)) (3.3.0)\n",
            "Collecting pathspec>=0.9.0\n",
            "  Downloading pathspec-0.11.1-py3-none-any.whl (29 kB)\n",
            "Collecting tokenize-rt>=3.2.0\n",
            "  Downloading tokenize_rt-5.0.0-py2.py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: ipython>=7.8.0 in /usr/local/lib/python3.10/dist-packages (from black->-r ./requirements.txt (line 4)) (7.34.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r ./requirements.txt (line 6)) (4.65.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->-r ./requirements.txt (line 6)) (2023.4.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r ./requirements.txt (line 6)) (2.27.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r ./requirements.txt (line 6)) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r ./requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->-r ./requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r ./requirements.txt (line 7)) (2.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0.dev0->-r ./requirements.txt (line 9)) (2022.10.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.0.dev0->-r ./requirements.txt (line 9)) (3.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m116.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from gradio->-r ./requirements.txt (line 10)) (4.5.0)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r ./requirements.txt (line 10)) (2.2.0)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiofiles\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting gradio-client>=0.1.3\n",
            "  Downloading gradio_client-0.2.0-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.9/287.9 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio->-r ./requirements.txt (line 10)) (2.1.2)\n",
            "Collecting semantic-version\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r ./requirements.txt (line 10)) (4.2.2)\n",
            "Collecting mdit-py-plugins<=0.3.3\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio->-r ./requirements.txt (line 10)) (1.10.7)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.24.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio->-r ./requirements.txt (line 10)) (3.7.1)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r ./requirements.txt (line 10)) (2.14.0)\n",
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio->-r ./requirements.txt (line 10)) (3.1.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio->-r ./requirements.txt (line 10)) (8.4.0)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.95.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r ./requirements.txt (line 12)) (3.20.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r ./requirements.txt (line 12)) (67.7.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.22.1-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.1/203.1 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r ./requirements.txt (line 10)) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r ./requirements.txt (line 10)) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r ./requirements.txt (line 10)) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r ./requirements.txt (line 6)) (23.1.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r ./requirements.txt (line 6)) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->-r ./requirements.txt (line 4)) (0.1.6)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->-r ./requirements.txt (line 4)) (5.7.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->-r ./requirements.txt (line 4)) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->-r ./requirements.txt (line 4)) (3.0.38)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->-r ./requirements.txt (line 4)) (4.4.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->-r ./requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black->-r ./requirements.txt (line 4)) (0.7.5)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->-r ./requirements.txt (line 10)) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r ./requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r ./requirements.txt (line 6)) (2022.7.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r ./requirements.txt (line 6)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r ./requirements.txt (line 6)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r ./requirements.txt (line 6)) (1.26.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate->-r ./requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate->-r ./requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->accelerate->-r ./requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate->-r ./requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->accelerate->-r ./requirements.txt (line 1)) (16.0.2)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r ./requirements.txt (line 10)) (1.3.0)\n",
            "Collecting httpcore<0.18.0,>=0.15.0\n",
            "  Downloading httpcore-0.17.0-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r ./requirements.txt (line 10)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r ./requirements.txt (line 10)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r ./requirements.txt (line 10)) (1.4.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r ./requirements.txt (line 10)) (1.0.7)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r ./requirements.txt (line 10)) (4.39.3)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio->-r ./requirements.txt (line 10)) (3.6.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.8.0->black->-r ./requirements.txt (line 4)) (0.8.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r ./requirements.txt (line 10)) (0.19.3)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.8.0->black->-r ./requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.8.0->black->-r ./requirements.txt (line 4)) (0.2.6)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->accelerate->-r ./requirements.txt (line 1)) (1.3.0)\n",
            "Building wheels for collected packages: fire, peft, transformers, ffmpy, pathtools\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116952 sha256=24e85363e928be3fe0a89e2afda14af3c5166761635e1efc3fecdd7df2ca662c\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=56175 sha256=e12db8b6d15e3c48a6b34abccc8589e9fb63b7799c5761582a994d98a2624849\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qpn79zpw/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.29.0.dev0-py3-none-any.whl size=7012426 sha256=1025a9415421b2754361d3f89f32b62c4a667f0d31905564bc7b4f0ea29ffa50\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qpn79zpw/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4707 sha256=8785fb63b006bb9a7b9fa89a36e9192117cf2d70d5cc537e879f9d29a9c55875\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=45656d54abdc9c805c6f3fd8c3951b5548b907fb317dcf5159e9853d8bfa410e\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built fire peft transformers ffmpy pathtools\n",
            "Installing collected packages: tokenizers, sentencepiece, pydub, pathtools, ffmpy, bitsandbytes, xxhash, websockets, uc-micro-py, tokenize-rt, smmap, setproctitle, sentry-sdk, semantic-version, python-multipart, pathspec, orjson, mypy-extensions, multidict, jedi, h11, frozenlist, fire, docker-pycreds, dill, async-timeout, aiofiles, yarl, uvicorn, starlette, responses, multiprocess, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, gitdb, black, aiosignal, transformers, httpx, GitPython, fastapi, aiohttp, wandb, gradio-client, gradio, datasets, accelerate, peft\n",
            "Successfully installed GitPython-3.1.31 accelerate-0.18.0 aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 bitsandbytes-0.38.1 black-23.3.0 datasets-2.12.0 dill-0.3.6 docker-pycreds-0.4.0 fastapi-0.95.1 ffmpy-0.3.0 fire-0.5.0 frozenlist-1.3.3 gitdb-4.0.10 gradio-3.28.3 gradio-client-0.2.0 h11-0.14.0 httpcore-0.17.0 httpx-0.24.0 huggingface-hub-0.14.1 jedi-0.18.2 linkify-it-py-2.0.2 mdit-py-plugins-0.3.3 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 orjson-3.8.12 pathspec-0.11.1 pathtools-0.1.2 peft-0.4.0.dev0 pydub-0.25.1 python-multipart-0.0.6 responses-0.18.0 semantic-version-2.10.0 sentencepiece-0.1.99 sentry-sdk-1.22.1 setproctitle-1.3.2 smmap-5.0.0 starlette-0.26.1 tokenize-rt-5.0.0 tokenizers-0.13.3 transformers-4.29.0.dev0 uc-micro-py-1.0.2 uvicorn-0.22.0 wandb-0.15.2 websockets-11.0.3 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxEXiXZccRbk",
        "outputId": "242216f8-fd57-4cc5-ac7a-29df7bce07b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\t\t\t\t generate.py\t      requirements.txt\n",
            "data\t\t\t\t infer_literature.py  scripts\n",
            "data-literature\t\t\t infer.py\t      templates\n",
            "export_hf_checkpoint.py\t\t LICENSE\t      utils\n",
            "export_state_dict_checkpoint.py  README_EN.md\n",
            "finetune.py\t\t\t README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "lora放到Huatuo-Llama-Med-Chinese项目根目录就行了"
      ],
      "metadata": {
        "id": "-jZsyC5cfKIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/Colab Notebooks\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3z7SFhV5ua-",
        "outputId": "c12425d2-6ec3-4acf-e4c7-50b82f1ebdd1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdvertiseGen.tar.gz\t\t       huatuo-llama\t  Untitled0.ipynb\n",
            "“Chinese-Vicuna-generate.ipynb”的副本  llama_model.ipynb\n",
            "“Colab_ChatGLM.ipynb”的副本\t       Untitled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv \"/content/drive/MyDrive/Colab Notebooks\"/huatuo-llama/lora* ."
      ],
      "metadata": {
        "id": "K10uNpsWVzhf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeSt8CxnbJ_g",
        "outputId": "0f74da61-953b-465e-dbf2-b6c479b83310"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\t\t\t\t infer_literature.py\t  README_EN.md\n",
            "data\t\t\t\t infer.py\t\t  README.md\n",
            "data-literature\t\t\t LICENSE\t\t  requirements.txt\n",
            "export_hf_checkpoint.py\t\t lora-alpaca-med\t  scripts\n",
            "export_state_dict_checkpoint.py  lora-alpaca-med-alldata  templates\n",
            "finetune.py\t\t\t lora-llama-literature\t  utils\n",
            "generate.py\t\t\t lora-llama-med\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#基于医学知识库\n",
        "!bash ./scripts/infer.sh   # 先改一下--load_8bit True \\，不然CUDA OOM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kBhoZXGXw3O",
        "outputId": "0f78f68e-8109-45b9-a120-39ae74fec43b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-08 05:58:40.066538: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-31d2fplm88l13 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n",
            "Loading checkpoint shards: 100% 33/33 [01:06<00:00,  2.01s/it]\n",
            "using lora ./lora-llama-med\n",
            "###infering###\n",
            "###instruction###\n",
            "小张最近感觉身体不适，出现心悸、气促等症状。体检发现心脏扩大、搏动减弱。\n",
            "###golden output###\n",
            "小张可能患有心肌炎，建议进行心电图和心脏超声等检查来确定诊断。治疗方案包括使用泼尼松、生脉饮和三磷酸腺苷等药物，同时建议适当控制体温，保持良好的营养状况。\n",
            "###model output###\n",
            "小张可能患有心血病。建议到医院进行检查和治疗。\n",
            "###infering###\n",
            "###instruction###\n",
            "小王被确诊为肝炎双重感染，最可能的并发症是什么？\n",
            "###golden output###\n",
            "最可能的并发症是肝癌和肝硬化。\n",
            "###model output###\n",
            "肝炎\n",
            "###infering###\n",
            "###instruction###\n",
            "麻风病和儿童哮喘的病因是否一致？\n",
            "###golden output###\n",
            "不一致，麻风病的病因是麻风分枝杆菌，而儿童哮喘的病因是气候、药物、吸入过敏原等。\n",
            "###model output###\n",
            "不一致。麻风病的病因是麻风，而儿童哮喘的病因是气胸。\n",
            "###infering###\n",
            "###instruction###\n",
            "一位患者出现了持续性的胀痛和黄疸，经影像学检查后确诊为肝胆管结石病，应该采取哪些治疗方案？\n",
            "###golden output###\n",
            "可以考虑采用残石的处理、微创保肝取石术或手术治疗等治疗方案。\n",
            "###model output###\n",
            "针对肝胆管结石病的治疗方案包括药物治疗和手术治疗。\n",
            "###infering###\n",
            "###instruction###\n",
            "一位35岁女性患者出现了关节疼痛和糖耐量异常，请问她可能患了什么疾病？\n",
            "###golden output###\n",
            "该患者可能患上了慢性自身免疫性胰腺炎，伴有慢性风湿性关节炎和糖耐量异常的症状。建议到消化内科进行检查和治疗。\n",
            "###model output###\n",
            "该患者可能患有TypeII胰腺炎。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#基于医学文献\n",
        "#单轮，除了要加8bit还要把\"\"\"去掉，sh不支持\"\"\"\n",
        "!bash ./scripts/infer-literature-single.sh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFJ7kWaM7v64",
        "outputId": "bff6430a-c52b-43fb-f1fe-844929f6b092"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-08 06:07:24.791177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-31d2fplm88l13 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n",
            "Loading checkpoint shards: 100% 33/33 [01:11<00:00,  2.15s/it]\n",
            "using lora ./lora-llama-literature\n",
            "instruction: 肝癌是什么？有哪些症状和迹象？\n",
            "Response: 肝癌是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿瘤，它是一种恶性肿��\n",
            "instruction: 肝癌是如何诊断的？有哪些检查和测试可以帮助诊断？\n",
            "Response: 肝癌的诊断常常包括血液检查、肝脏超声、CT、MRI、血液检查、肝脏CT、MRI、血液检查、肝脏CT、MRI、血液检查、肝脏CT、MRI、血液检查、肝脏CT、MRI、血液检查、肝脏CT、MRI、血液检查、肝脏CT、MRI、血液检查、肝脏CT、MRI、血液检查、肝脏CT、MRI、血液检查、肝脏CT、MRI、血液检查、肝\n",
            "instruction: Sorafenib是一种口服的多靶点酪氨酸激酶抑制剂，它的作用机制是什么？\n",
            "Response: Sorafenib是一种口服的多靶点酪氨酸激酶抑制剂，其作用机制主要是通过抑制肿瘤细胞的VEGF-VEGFR信号通路，从而抑制肿瘤细胞的生长和扩散。\n",
            "instruction: Regorafenib是一种口服的多靶点酪氨酸激酶抑制剂，它的作用机制是什么？它和Sorafenib有什么不同？\n",
            "Response: Regorafenib是一种多靶点酪氨酸激酶抑制剂，它的作用机制是通过阻塞肝癌细胞增殖、抑制肝癌细胞增殖、抑制肝癌细胞凋亡和抑制肝癌细胞周期来抑制肝癌的进展。它和Sorafenib的不同之处在于，Regorafenib是一种多靶点酪氨酸激酶抑制剂，而Sorafenib则是一种单靶点酪氨酸激酶抑制剂。\n",
            "instruction: 肝癌药物治疗的副作用有哪些？如何缓解这些副作用？\n",
            "Response: 肝癌药物治疗的副作用主要包括恶心、免疫抑制、血液循环阻滞、肝脏损伤、食欲减退、肝区疼痛、恶心、呕吐等。针对这些副作用，我们可以采取一些缓解措施，例如:\n",
            "instruction: 肝癌药物治疗的费用高昂，如何降低治疗的经济负担？\n",
            "Response: 肝癌药物治疗的费用高昂，可以采用经济负担的降低方法。一种降低经济负担的方法是通过改变治疗方案来降低治疗的经济负担。另一种降低治疗的经济负担的方法是通过改变治疗方案来降低治疗的经济负担。另一种降低治疗的经济负担的方法是通过改变治疗方案来降低治疗的经济负担。\n",
            "instruction: 我想了解一下β-谷甾醇是否可作为肝癌的治疗药物\n",
            "Response: β-谷甾醇是一种有潜力成为治疗肝癌的药物，但仍需要进一步的研究和临床试验。\n",
            "instruction: 能介绍一下最近Hsa＿circ＿0008583在肝细胞癌治疗中的潜在应用的研究么?\n",
            "Response: 是的，最近Hsa＿circ＿0008583在肝细胞癌治疗中的潜在应用？研究表明，最近Hsa＿circ＿0008583在肝细胞癌治疗中具有抑制肝癌细胞增殖、抑制肝癌细胞凋亡、抑制肝癌细胞侵袭、防止肝癌细胞侵袭、防止肝癌细胞侵袭、防止肝癌细胞侵袭、防止肝癌细胞侵袭、防\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#基于医学文献\n",
        "#多轮，除了要加8bit还要把\"\"\"去掉，sh不支持\"\"\"\n",
        "!bash ./scripts/infer-literature-multi.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rshHm45v765b",
        "outputId": "6848f42b-c74b-4803-f6fd-7241b8ba441b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-08 06:47:53.494117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-31d2fplm88l13 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/Huatuo-Llama-Med-Chinese/\u001b[0m\u001b[1;33minfer_literature.py\u001b[0m:\u001b[94m128\u001b[0m in \u001b[92m<module>\u001b[0m        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m125 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m126 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m127 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m\"\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m\"\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m128 \u001b[2m│   \u001b[0mfire.Fire(main)                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m129 \u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/fire/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m141\u001b[0m in \u001b[92mFire\u001b[0m             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   \u001b[0mcontext.update(caller_globals)                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   \u001b[0mcontext.update(caller_locals)                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m\u001b[2m  \u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m141 \u001b[2m  \u001b[0mcomponent_trace = _Fire(component, args, parsed_flag_args, context,  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m142 \u001b[0m\u001b[2m  \u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m143 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mif\u001b[0m component_trace.HasError():                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m144 \u001b[0m\u001b[2m│   \u001b[0m_DisplayError(component_trace)                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/fire/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m475\u001b[0m in \u001b[92m_Fire\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m472 \u001b[0m\u001b[2m│     \u001b[0mis_class = inspect.isclass(component)                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m473 \u001b[0m\u001b[2m│     \u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m474 \u001b[0m\u001b[2m│     \u001b[0m\u001b[94mtry\u001b[0m:                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m475 \u001b[2m│   │   \u001b[0mcomponent, remaining_args = _CallAndUpdateTrace(               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m476 \u001b[0m\u001b[2m│   │   │   \u001b[0mcomponent,                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m477 \u001b[0m\u001b[2m│   │   │   \u001b[0mremaining_args,                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m478 \u001b[0m\u001b[2m│   │   │   \u001b[0mcomponent_trace,                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/fire/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m691\u001b[0m in                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_CallAndUpdateTrace\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m688 \u001b[0m\u001b[2m│   \u001b[0mloop = asyncio.get_event_loop()                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m689 \u001b[0m\u001b[2m│   \u001b[0mcomponent = loop.run_until_complete(fn(*varargs, **kwargs))        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m690 \u001b[0m\u001b[2m  \u001b[0m\u001b[94melse\u001b[0m:                                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m691 \u001b[2m│   \u001b[0mcomponent = fn(*varargs, **kwargs)                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m692 \u001b[0m\u001b[2m  \u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m693 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mif\u001b[0m treatment == \u001b[33m'\u001b[0m\u001b[33mclass\u001b[0m\u001b[33m'\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m694 \u001b[0m\u001b[2m│   \u001b[0maction = trace.INSTANTIATED_CLASS                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/content/Huatuo-Llama-Med-Chinese/\u001b[0m\u001b[1;33minfer_literature.py\u001b[0m:\u001b[94m39\u001b[0m in \u001b[92mmain\u001b[0m             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 36 \u001b[0m):                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 37 \u001b[0m\u001b[2m│   \u001b[0mprompter = Prompter(prompt_template)                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 38 \u001b[0m\u001b[2m│   \u001b[0mtokenizer = LlamaTokenizer.from_pretrained(base_model)             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 39 \u001b[2m│   \u001b[0mmodel = LlamaForCausalLM.from_pretrained(                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 40 \u001b[0m\u001b[2m│   │   \u001b[0mbase_model,                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 41 \u001b[0m\u001b[2m│   │   \u001b[0mload_in_8bit=load_8bit,                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 42 \u001b[0m\u001b[2m│   │   \u001b[0mtorch_dtype=torch.float16,                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m2730\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2727 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mkey: device_map[key] \u001b[94mfor\u001b[0m key \u001b[95min\u001b[0m device_map.keys() \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2728 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m}                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2729 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m device_map_without_lm_head.values() \u001b[95mor\u001b[0m \u001b[33m\"\u001b[0m\u001b[33md\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2730 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2731 \u001b[0m\u001b[2;90m│   │   │   │   │   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2732 \u001b[0m\u001b[2;33m│   │   │   │   │   │   \u001b[0m\u001b[33mSome modules are dispatched on the CPU or the\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m2733 \u001b[0m\u001b[2;33m│   │   │   │   │   │   \u001b[0m\u001b[33mthe quantized model. If you want to dispatch \u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mValueError: \u001b[0m\n",
            "                        Some modules are dispatched on the CPU or the disk. Make\n",
            "sure you have enough GPU RAM to fit\n",
            "                        the quantized model. If you want to dispatch the model \n",
            "on the CPU or the disk while keeping\n",
            "                        these modules in \u001b[1;36m32\u001b[0m-bit, you need to set \n",
            "`\u001b[33mload_in_8bit_fp32_cpu_offload\u001b[0m=\u001b[3;92mTrue\u001b[0m` and pass a custom\n",
            "                        `device_map` to `from_pretrained`. Check\n",
            "                        \u001b[4;94mhttps://huggingface.co/docs/transformers/main/en/main_cl\u001b[0m\n",
            "\u001b[4;94masses/quantization#offload-between-cpu-and-gpu\u001b[0m\n",
            "                        for more details.\n",
            "                        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./scripts/test.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mafrkEXiChE6",
        "outputId": "65e7437d-0bef-498b-8dba-8233e5ef9564"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ori\n",
            "./scripts/test.sh: line 29: infer_result/o_tmp.txt: No such file or directory\n",
            "alpaca\n",
            "./scripts/test.sh: line 31: infer_result/a_tmp.txt: No such file or directory\n",
            "med\n",
            "./scripts/test.sh: line 33: infer_result/m_tmp.txt: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPmnfgJcQlNm",
        "outputId": "64ad2612-83b5-416c-fa4b-4ebeb9ccefcc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assets\t\t\t\t infer_literature.py\t  lora-llama-med.zip\n",
            "data\t\t\t\t infer.py\t\t  README_EN.md\n",
            "data-literature\t\t\t LICENSE\t\t  README.md\n",
            "export_hf_checkpoint.py\t\t lora-alpaca-med\t  requirements.txt\n",
            "export_state_dict_checkpoint.py  lora-alpaca-med-alldata  scripts\n",
            "finetune.py\t\t\t lora-llama-literature\t  templates\n",
            "generate.py\t\t\t lora-llama-med\t\t  utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from infer_api import *"
      ],
      "metadata": {
        "id": "irmNipdlQ0mG"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 783
        },
        "id": "-nuR5K2-Q0jC",
        "outputId": "fc4f4bba-77e1-4440-bbb5-2e0f75fc93dd"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \n",
            "The class this function is called from is 'LlamaTokenizer'.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/content/Huatuo-Llama-Med-Chinese/\u001b[0m\u001b[1;33minfer_api.py\u001b[0m:\u001b[94m5\u001b[0m in \u001b[92mapi\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mapi\u001b[0m(q=\u001b[33m\"\u001b[0m\u001b[33m肝癌有哪些症状？\u001b[0m\u001b[33m\"\u001b[0m):                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m5 \u001b[2m│   \u001b[0mres = main(load_8bit=\u001b[94mTrue\u001b[0m, base_model=\u001b[33m'\u001b[0m\u001b[33mdecapoda-research/llama-7b-hf\u001b[0m\u001b[33m'\u001b[0m, lora_weights=     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m\u001b[2m│   │    \u001b[0minstruct_dir=\u001b[33m'\u001b[0m\u001b[33m./data/infer.json\u001b[0m\u001b[33m'\u001b[0m, prompt_template=\u001b[33m'\u001b[0m\u001b[33mmed_template\u001b[0m\u001b[33m'\u001b[0m,use_instant=\u001b[94mTr\u001b[0m     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m7 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m res                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m8 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/content/Huatuo-Llama-Med-Chinese/\u001b[0m\u001b[1;33minfer.py\u001b[0m:\u001b[94m41\u001b[0m in \u001b[92mmain\u001b[0m                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 38 \u001b[0m):                                                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 39 \u001b[0m\u001b[2m│   \u001b[0mprompter = Prompter(prompt_template)                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 40 \u001b[0m\u001b[2m│   \u001b[0mtokenizer = LlamaTokenizer.from_pretrained(base_model)                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 41 \u001b[2m│   \u001b[0mmodel = LlamaForCausalLM.from_pretrained(                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 42 \u001b[0m\u001b[2m│   │   \u001b[0mbase_model,                                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 43 \u001b[0m\u001b[2m│   │   \u001b[0mload_in_8bit=load_8bit,                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 44 \u001b[0m\u001b[2m│   │   \u001b[0mtorch_dtype=torch.float16,                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m2730\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2727 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mkey: device_map[key] \u001b[94mfor\u001b[0m key \u001b[95min\u001b[0m device_map.keys() \u001b[94mif\u001b[0m key \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m modu  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2728 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m}                                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2729 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mcpu\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m device_map_without_lm_head.values() \u001b[95mor\u001b[0m \u001b[33m\"\u001b[0m\u001b[33mdisk\u001b[0m\u001b[33m\"\u001b[0m \u001b[95min\u001b[0m device_map_  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2730 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mValueError\u001b[0m(                                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2731 \u001b[0m\u001b[2;90m│   │   │   │   │   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2732 \u001b[0m\u001b[2;33m│   │   │   │   │   │   \u001b[0m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure yo\u001b[0m  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2733 \u001b[0m\u001b[2;33m│   │   │   │   │   │   \u001b[0m\u001b[33mthe quantized model. If you want to dispatch the model on the CP\u001b[0m  \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mValueError: \u001b[0m\n",
              "                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to \n",
              "fit\n",
              "                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n",
              "                        these modules in \u001b[1;36m32\u001b[0m-bit, you need to set `\u001b[33mload_in_8bit_fp32_cpu_offload\u001b[0m=\u001b[3;92mTrue\u001b[0m` and pass a \n",
              "custom\n",
              "                        `device_map` to `from_pretrained`. Check\n",
              "                        \u001b[4;94mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-\u001b[0m\n",
              "\u001b[4;94mcpu-and-gpu\u001b[0m\n",
              "                        for more details.\n",
              "                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/content/Huatuo-Llama-Med-Chinese/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">infer_api.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">api</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">api</span>(q=<span style=\"color: #808000; text-decoration-color: #808000\">\"肝癌有哪些症状？\"</span>):                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>5 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>res = main(load_8bit=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, base_model=<span style=\"color: #808000; text-decoration-color: #808000\">'decapoda-research/llama-7b-hf'</span>, lora_weights=     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">6 │   │    </span>instruct_dir=<span style=\"color: #808000; text-decoration-color: #808000\">'./data/infer.json'</span>, prompt_template=<span style=\"color: #808000; text-decoration-color: #808000\">'med_template'</span>,use_instant=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">Tr</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">7 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> res                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">8 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/content/Huatuo-Llama-Med-Chinese/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">infer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">41</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">main</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 38 </span>):                                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 39 │   </span>prompter = Prompter(prompt_template)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 40 │   </span>tokenizer = LlamaTokenizer.from_pretrained(base_model)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 41 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model = LlamaForCausalLM.from_pretrained(                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 42 │   │   </span>base_model,                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 43 │   │   </span>load_in_8bit=load_8bit,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 44 │   │   </span>torch_dtype=torch.float16,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2730</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2727 │   │   │   │   │   </span>key: device_map[key] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> key <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> device_map.keys() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> key <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> modu  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2728 │   │   │   │   </span>}                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2729 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"cpu\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> device_map_without_lm_head.values() <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #808000; text-decoration-color: #808000\">\"disk\"</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> device_map_  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2730 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">ValueError</span>(                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2731 </span><span style=\"color: #bfbfbf; text-decoration-color: #bfbfbf\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2732 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">Some modules are dispatched on the CPU or the disk. Make sure yo</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2733 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">the quantized model. If you want to dispatch the model on the CP</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>\n",
              "                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to \n",
              "fit\n",
              "                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n",
              "                        these modules in <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>-bit, you need to set `<span style=\"color: #808000; text-decoration-color: #808000\">load_in_8bit_fp32_cpu_offload</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>` and pass a \n",
              "custom\n",
              "                        `device_map` to `from_pretrained`. Check\n",
              "                        <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-</span>\n",
              "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">cpu-and-gpu</span>\n",
              "                        for more details.\n",
              "                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(input):\n",
        "    if input:\n",
        "        reply = api(input)\n",
        "        return reply\n",
        "\n",
        "inputs = gr.inputs.Textbox(lines=7, label=\"Chat with AI\")\n",
        "outputs = gr.outputs.Textbox(label=\"Reply\")\n",
        "\n",
        "gr.Interface(fn=chatbot, inputs=inputs, outputs=outputs, title=\"LLaMA Chatbot\",\n",
        "             description=\"Ask anything you want\",\n",
        "             theme=\"compact\").launch(share=True,server_port=5000)"
      ],
      "metadata": {
        "id": "pOZyb-4qJp_6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "5ce20080-42ec-4202-eed3-fa51eac921ed"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gradio/inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gradio/blocks.py:636: UserWarning: Cannot load compact. Caught Exception: The space compact does not exist\n",
            "  warnings.warn(f\"Cannot load {theme}. Caught Exception: {str(e)}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://1bbb809d6b77e3aef6.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1bbb809d6b77e3aef6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ky4faEgQIlCT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}